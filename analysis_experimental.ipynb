{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Setup and Data Loading\n",
    "\t•\tImport necessary libraries.\n",
    "\t•\tLoad the model and NetworkTrace.\n",
    "\t•\tConvert traces to DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "from traceable_model import TraceableModel\n",
    "from sample_models import IrisNN\n",
    "from trace_nn import NetworkTrace\n",
    "from model_config import config_iris\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main process in main.py to execute the training loop and save the model's learnable parameters\n",
    "subprocess.run([\"python\", \"main.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model, configuration variables, and NetworkTrace.\n",
    "\n",
    "input_size = config_iris['input_size']\n",
    "hidden_size = config_iris['hidden_size']\n",
    "output_size = config_iris['output_size']\n",
    "num_epochs = config_iris['num_epochs']\n",
    "batch_size = config_iris['batch_size']\n",
    "# Initialize the base model\n",
    "base_model = IrisNN() \n",
    "\n",
    "# Initialize the network trace\n",
    "network_trace = NetworkTrace()\n",
    "\n",
    "# Define layers to trace using config_iris\n",
    "layers_to_trace = list(config_iris['layer_names'].values())\n",
    "\n",
    "# Wrap the base model with TraceableModel\n",
    "model = TraceableModel(base_model, network_trace, layers_to_trace=layers_to_trace)\n",
    "\n",
    "# Load the trained model's state\n",
    "model.load_state_dict(torch.load('outputs/trained_model_full.pth'))\n",
    "\n",
    "\n",
    "# Load the saved network trace from the pickle file (weights and biases from training)\n",
    "with open('outputs/network_trace.pkl', 'rb') as f:\n",
    "    network_trace = pickle.load(f)\n",
    "\n",
    "    # Assign the loaded network trace back to the model\n",
    "model.network_trace = network_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch, neurons in network_trace.trace.items():\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for global_neuron_id, neuron_trace in neurons.items():\n",
    "        print(f\"  Global Neuron ID: {global_neuron_id}\")\n",
    "        print(f\"    Layer: {neuron_trace.layer_name}\")\n",
    "        print(f\"    Activations: {neuron_trace.get_activation_statistics()}\")\n",
    "        print(f\"    Weight: {neuron_trace.weight}\")\n",
    "        print(f\"    Bias: {neuron_trace.bias}\")\n",
    "        print(f\"    Activation Sum: {neuron_trace.activation_sum}\")\n",
    "        print(f\"    Activation Squared Sum: {neuron_trace.activation_squared_sum}\")\n",
    "        print(f\"    Activation Max: {neuron_trace.activation_max}\")\n",
    "        print(f\"    Activation Min: {neuron_trace.activation_min}\")\n",
    "        print(f\"    Count: {neuron_trace.count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert network_trace to a DataFrame using the class method\n",
    "network_df = network_trace.neurons_to_dataframe()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"DataFrame columns:\")\n",
    "print(network_df.columns)\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "print(network_df.head())\n",
    "\n",
    "# Display dataframe\n",
    "network_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Weights and Building Network Graph:\n",
    "•\tUse connections_df to reconstruct the network.\n",
    "•\tVisualize and analyze connection strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert connections to DataFrame\n",
    "connections_df = network_trace.connections_to_dataframe()\n",
    "print(\"Connections DataFrame:\")\n",
    "print(connections_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the Network Graph\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for _, row in connections_df.iterrows():\n",
    "    source = f\"{row['source_layer']}_{row['source_neuron']}\"\n",
    "    target = f\"{row['target_layer']}_{row['target_neuron']}\"\n",
    "    weight = row['weight']\n",
    "    G.add_edge(source, target, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fixing connections_to_dataframe\n",
    "connections_df = network_trace.connections_to_dataframe()\n",
    "print(\"Connections DataFrame:\")\n",
    "print(connections_df.head())\n",
    "print(\"Columns:\", connections_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming connections_df is your DataFrame\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for _, row in connections_df.iterrows():\n",
    "    source = f\"{row['source_layer']}_{row['source_neuron']}\"\n",
    "    target = f\"{row['target_layer']}_{row['target_neuron']}\"\n",
    "    weight = row['weight']\n",
    "    G.add_edge(source, target, weight=weight)\n",
    "\n",
    "# Assign positions to nodes in 3D space\n",
    "pos = {}\n",
    "layer_positions = {'L_input': 0, 'L_hidden_1': 1, 'L_hidden_2': 2, 'L_output': 3}\n",
    "for node in G.nodes():\n",
    "    layer, neuron = node.rsplit('_', 1)\n",
    "    x = int(neuron)\n",
    "    y = layer_positions.get(layer, 0)\n",
    "    z = 0  # You can use activation values or other metrics here\n",
    "    pos[node] = (x, y, z)\n",
    "\n",
    "# Extract edge coordinates\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "edge_z = []\n",
    "for edge in G.edges():\n",
    "    x0, y0, z0 = pos[edge[0]]\n",
    "    x1, y1, z1 = pos[edge[1]]\n",
    "    edge_x.extend([x0, x1, None])\n",
    "    edge_y.extend([y0, y1, None])\n",
    "    edge_z.extend([z0, z1, None])\n",
    "\n",
    "# Extract node coordinates\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_z = []\n",
    "for node in G.nodes():\n",
    "    x, y, z = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_z.append(z)\n",
    "\n",
    "# Create the Plotly figure\n",
    "edge_trace = go.Scatter3d(\n",
    "    x=edge_x, y=edge_y, z=edge_z,\n",
    "    line=dict(width=0.5, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines')\n",
    "\n",
    "node_trace = go.Scatter3d(\n",
    "    x=node_x, y=node_y, z=node_z,\n",
    "    mode='markers+text',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='blue'),\n",
    "    text=list(G.nodes()),\n",
    "    hoverinfo='text')\n",
    "\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                layout=go.Layout(\n",
    "                    title='3D Neural Network Graph',\n",
    "                    showlegend=False,\n",
    "                    scene=dict(\n",
    "                        xaxis_title='Neuron Index',\n",
    "                        yaxis_title='Layer',\n",
    "                        zaxis_title='Activation Value'),\n",
    "                    margin=dict(l=0, r=0, b=0, t=50)))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Your connections_df DataFrame\n",
    "# ... (Use your actual DataFrame here)\n",
    "\n",
    "# Filter for the epoch you want to visualize\n",
    "epoch_to_visualize = 0\n",
    "df = connections_df[connections_df['epoch'] == epoch_to_visualize]\n",
    "\n",
    "# Create unique node labels\n",
    "df['source_node'] = df['source_layer'] + '_' + df['source_neuron'].astype(str)\n",
    "df['target_node'] = df['target_layer'] + '_' + df['target_neuron'].astype(str)\n",
    "\n",
    "# Combine nodes and sort them\n",
    "nodes = list(set(df['source_node'].unique()).union(set(df['target_node'].unique())))\n",
    "\n",
    "def sort_key(node_label):\n",
    "    layer, neuron = node_label.rsplit('_', 1)\n",
    "    neuron = int(neuron)\n",
    "    layer_order = {'L_input': 0, 'L_hidden_1': 1, 'L_hidden_2': 2, 'L_output': 3}\n",
    "    return (layer_order.get(layer, 99), neuron)\n",
    "\n",
    "nodes_sorted = sorted(nodes, key=sort_key)\n",
    "node_indices = {node: idx for idx, node in enumerate(nodes_sorted)}\n",
    "\n",
    "# Update indices in df\n",
    "df['source_idx'] = df['source_node'].map(node_indices)\n",
    "df['target_idx'] = df['target_node'].map(node_indices)\n",
    "\n",
    "# Prepare source, target, and weight lists\n",
    "source_indices = df['source_idx'].tolist()\n",
    "target_indices = df['target_idx'].tolist()\n",
    "weights = df['weight'].abs().tolist()  # Use absolute values for the flows\n",
    "\n",
    "# Assign colors to layers\n",
    "layer_colors = {\n",
    "    'L_input': 'lightblue',\n",
    "    'L_hidden_1': 'lightgreen',\n",
    "    'L_hidden_2': 'orange',\n",
    "    'L_output': 'red'\n",
    "}\n",
    "\n",
    "# Map node colors based on their layer\n",
    "node_colors = [layer_colors.get(node.split('_')[0], 'gray') for node in nodes_sorted]\n",
    "\n",
    "# Normalize weights for opacity\n",
    "max_weight = max(weights)\n",
    "normalized_weights = [0.2 + 0.8 * (w / max_weight) for w in weights]\n",
    "\n",
    "# Link colors with opacity\n",
    "link_colors = [\n",
    "    f'rgba(0,0,255,{opacity})' if w >= 0 else f'rgba(255,0,0,{opacity})'\n",
    "    for w, opacity in zip(df['weight'], normalized_weights)\n",
    "]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    arrangement='snap',  # Try 'perpendicular', 'freeform', 'fixed', or 'snap'\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color='black', width=0.5),\n",
    "        label=nodes_sorted,\n",
    "        color=node_colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source_indices,\n",
    "        target=target_indices,\n",
    "        value=weights,\n",
    "        color=link_colors,\n",
    "        customdata=df['weight'].tolist(),\n",
    "        hovertemplate='Source: %{source.label}<br />Target: %{target.label}<br />Weight: %{customdata}<extra></extra>'\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title_text='Neural Network Sankey Diagram - Epoch {}'.format(epoch_to_visualize), font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\n",
    "\n",
    "# Assuming connections_df is your DataFrame\n",
    "# Build the graph with edge attributes\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Populate the graph with nodes and edges\n",
    "for _, row in connections_df.iterrows():\n",
    "    source = f\"{row['source_layer']}_n_{row['source_neuron']}\"\n",
    "    target = f\"{row['target_layer']}_n_{row['target_neuron']}\"\n",
    "    weight = row['weight']\n",
    "    G.add_edge(source, target, weight=weight)\n",
    "\n",
    "# Generate 3D positions for nodes\n",
    "pos = nx.spring_layout(G, k=0.8, seed=42, dim=3)  # Set 'dim=3' for 3D positions\n",
    "\n",
    "# Set up 3D plot\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Extract node positions\n",
    "x_nodes = [pos[node][0] for node in G.nodes()]\n",
    "y_nodes = [pos[node][1] for node in G.nodes()]\n",
    "z_nodes = [pos[node][2] for node in G.nodes()]\n",
    "\n",
    "# Get edge weights and normalize for edge width\n",
    "weights = nx.get_edge_attributes(G, 'weight')\n",
    "edge_weights = list(weights.values())\n",
    "abs_edge_weights = [abs(w) for w in edge_weights]\n",
    "\n",
    "# Normalize edge widths for visualization\n",
    "max_width = 5\n",
    "min_width = 0.5\n",
    "edge_widths = [min_width + (max_width - min_width) * (abs(w) / max(abs_edge_weights)) for w in abs_edge_weights]\n",
    "\n",
    "# Edge colors based on weight sign\n",
    "edge_colors = ['red' if w > 0 else 'blue' for w in edge_weights]\n",
    "\n",
    "# Draw nodes\n",
    "ax.scatter(x_nodes, y_nodes, z_nodes, s=100, c='lightgray', edgecolors='k', depthshade=True)\n",
    "\n",
    "# Draw edges\n",
    "for i, (edge, weight) in enumerate(zip(G.edges(), edge_weights)):\n",
    "    x = [pos[edge[0]][0], pos[edge[1]][0]]\n",
    "    y = [pos[edge[0]][1], pos[edge[1]][1]]\n",
    "    z = [pos[edge[0]][2], pos[edge[1]][2]]\n",
    "    ax.plot(x, y, z, c=edge_colors[i], linewidth=edge_widths[i])\n",
    "\n",
    "# Add node labels\n",
    "for node in G.nodes():\n",
    "    x, y, z = pos[node]\n",
    "    ax.text(x, y, z, node, fontsize=8)\n",
    "\n",
    "# Create a custom legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='red', lw=2, label='Positive Weight'),\n",
    "    Line2D([0], [0], color='blue', lw=2, label='Negative Weight'),\n",
    "    Patch(facecolor='lightgray', edgecolor='k', label='Neuron')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "ax.set_title(\"3D Neural Network Graph with Edge Widths Representing Weight Magnitude\")\n",
    "ax.axis('off')  # Hide axes\n",
    "\n",
    "# Optionally, adjust the viewing angle\n",
    "ax.view_init(elev=20, azim=30)  # Adjust elevation and azimuthal angles\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Network\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(G, k=0.15)  # Adjust k for spacing\n",
    "edge_weights = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw(G, pos, with_labels=True, node_size=500, arrowsize=20)\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_weights)\n",
    "plt.title(\"Neural Network Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter connections with high absolute weight values\n",
    "threshold = 0.3  # Adjust threshold as needed\n",
    "strong_connections = connections_df[connections_df['weight'].abs() > threshold]\n",
    "print(\"Strong Connections:\")\n",
    "print(strong_connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Build the graph with edge attributes\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Populate the graph with nodes and edges\n",
    "for _, row in connections_df.iterrows():\n",
    "    source = f\"{row['source_layer']}_n_{row['source_neuron']}\"\n",
    "    target = f\"{row['target_layer']}_n_{row['target_neuron']}\"\n",
    "    weight = row['weight']\n",
    "    G.add_edge(source, target, weight=weight)\n",
    "\n",
    "# Increase 'k' for more node separation and spread out the graph\n",
    "pos = nx.spring_layout(G, k=0.8, seed=42)  # Adjusted 'k' for more separation\n",
    "\n",
    "# Get edge weights and normalize for edge width\n",
    "weights = nx.get_edge_attributes(G, 'weight')\n",
    "edge_weights = list(weights.values())\n",
    "abs_edge_weights = [abs(w) for w in edge_weights]\n",
    "\n",
    "# Normalize edge widths for visualization\n",
    "max_width = 5\n",
    "min_width = 0.5\n",
    "edge_widths = [min_width + (max_width - min_width) * (abs(w) / max(abs_edge_weights)) for w in abs_edge_weights]\n",
    "\n",
    "# Normalize edge colors based on weight magnitude\n",
    "edge_colors = ['red' if w > 0 else 'blue' for w in edge_weights]\n",
    "\n",
    "# Draw nodes with smaller size\n",
    "nx.draw_networkx_nodes(G, pos, node_size=400, node_color='lightgray')  # Reduced node size\n",
    "\n",
    "# Draw edges with varying widths and colors\n",
    "nx.draw_networkx_edges(G, pos, edge_color=edge_colors, width=edge_widths, arrowsize=20)\n",
    "\n",
    "# Draw node labels with smaller font size\n",
    "nx.draw_networkx_labels(G, pos, labels={node: node for node in G.nodes()}, font_size=8)  # Smaller font size\n",
    "\n",
    "# Create a custom legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='red', lw=2, label='Positive Weight'),\n",
    "    Line2D([0], [0], color='blue', lw=2, label='Negative Weight'),\n",
    "    Patch(facecolor='lightgray', edgecolor='k', label='Neuron')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "plt.title(\"Neural Network Graph with Edge Widths Representing Weight Magnitude\")\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associating Predictions with Model States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_config import config_iris\n",
    "\n",
    "# Start inference trace\n",
    "model.start_inference_trace(epoch='inference')\n",
    "\n",
    "# Example test data\n",
    "X_test = config_iris['data']['X_test']\n",
    "predictions = model.predict(X_test, batch_size=config_iris['inference_batch_size'], return_probabilities=True)\n",
    "\n",
    "# End inference trace\n",
    "model.end_inference_trace()\n",
    "\n",
    "# Access predictions\n",
    "predictions = network_trace.predictions['inference']  # NumPy array of predictions\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Access true labels\n",
    "y_test = config_iris['data']['y_test'].cpu().numpy().flatten()\n",
    "true_labels = y_test.astype(int)\n",
    "\n",
    "# Ensure that predicted_classes and true_labels have the same shape\n",
    "if predicted_classes.shape != true_labels.shape:\n",
    "    raise ValueError(f\"Shape mismatch: predicted_classes shape {predicted_classes.shape}, true_labels shape {true_labels.shape}\")\n",
    "\n",
    "# Determine correctness\n",
    "correct_predictions = (predicted_classes == true_labels).astype(int)\n",
    "\n",
    "# Optional: Print or log the results\n",
    "print(f\"Correct Predictions: {correct_predictions.sum()} out of {len(true_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run to clear inference trace after the session (optional)\n",
    "# model.network_trace.trace.pop('inference', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "num_samples = len(predictions)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample_data = {\n",
    "        'Sample_Index': i,\n",
    "        'Prediction': predicted_classes[i],\n",
    "        'True_Label': true_labels[i],\n",
    "        'Correct': correct_predictions[i]\n",
    "    }\n",
    "    for layer_name in network_trace.layer_order:\n",
    "        # Get the number of neurons in the layer\n",
    "        num_neurons = len([key for key in network_trace.global_neuron_id_map if key[0] == layer_name])\n",
    "        for neuron_index in range(num_neurons):\n",
    "            global_neuron_id = network_trace.global_neuron_id_map.get((layer_name, neuron_index))\n",
    "            if global_neuron_id is not None:\n",
    "                neuron_trace = network_trace.trace['inference'].get(global_neuron_id)\n",
    "                if neuron_trace is not None and i < len(neuron_trace.activations):\n",
    "                    activation_value = neuron_trace.activations[i]\n",
    "                    neuron_name = f\"{layer_name}_neuron_{neuron_index}\"\n",
    "                    sample_data[neuron_name] = activation_value\n",
    "    data.append(sample_data)\n",
    "\n",
    "# Create the DataFrame\n",
    "activation_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Separate data based on predicted classes\n",
    "num_classes = len(np.unique(predicted_classes))\n",
    "class_means = {}\n",
    "\n",
    "# Define the columns that correspond to neuron activations\n",
    "activation_columns = [col for col in activation_df.columns if 'neuron' in col]\n",
    "\n",
    "for cls in range(num_classes):\n",
    "    class_df = activation_df[activation_df['Prediction'] == cls]\n",
    "    class_means[cls] = class_df[activation_columns].mean()\n",
    "\n",
    "# Compare the means between all classes\n",
    "activation_diff = {}\n",
    "for i in range(num_classes):\n",
    "    for j in range(i + 1, num_classes):\n",
    "        diff_key = f\"class_{j}_minus_class_{i}\"\n",
    "        activation_diff[diff_key] = class_means[j] - class_means[i]\n",
    "\n",
    "# Convert activation_diff to DataFrame for easier viewing and sorting\n",
    "activation_diff_df = pd.DataFrame(activation_diff)\n",
    "\n",
    "print(\"Activation differences between predicted classes:\")\n",
    "print(activation_diff_df.sort_values(by=list(activation_diff_df.columns), ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "# Get unique predicted classes\n",
    "unique_classes = activation_df['Prediction'].unique()\n",
    "print(\"Unique predicted classes:\", unique_classes)\n",
    "\n",
    "# Create DataFrames for each class\n",
    "class_dfs = {}\n",
    "for cls in unique_classes:\n",
    "    df = activation_df[activation_df['Prediction'] == cls]\n",
    "    class_dfs[cls] = df\n",
    "    print(f\"Class {cls} DataFrame shape: {df.shape}\")\n",
    "\n",
    "# Calculate mean activations for each class\n",
    "activation_columns = [col for col in activation_df.columns if 'neuron' in col]\n",
    "class_means = {}\n",
    "for cls, df in class_dfs.items():\n",
    "    class_means[cls] = df[activation_columns].mean()\n",
    "\n",
    "# Compute activation differences between class pairs\n",
    "activation_diffs = {}\n",
    "for cls1, cls2 in combinations(unique_classes, 2):\n",
    "    diff = class_means[cls1] - class_means[cls2]\n",
    "    key = f\"{cls1}-{cls2}\"\n",
    "    activation_diffs[key] = diff\n",
    "\n",
    "# Choose a class pair to analyze\n",
    "pair_key = '0-1'  # Adjust based on your classes\n",
    "activation_diff = activation_diffs.get(pair_key)\n",
    "\n",
    "if activation_diff is not None and not activation_diff.empty:\n",
    "    # Get the top neurons with the largest activation differences\n",
    "    top_neurons = activation_diff.abs().sort_values(ascending=False).head(10).index\n",
    "    activation_diff_top = activation_diff[top_neurons]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    activation_diff_top.plot(kind='bar')\n",
    "    plt.title(f'Activation Difference Between Predicted Classes {pair_key} for Top Neurons')\n",
    "    plt.ylabel(f'Activation Difference (Class {pair_key.replace(\"-\", \" vs \")})')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"No data available for class pair {pair_key}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Print the activation_diff dictionary\n",
    "print(activation_diff)\n",
    "\n",
    "# Assuming activation_diff should only contain scalar values, you can filter out non-scalar entries\n",
    "activation_diff_filtered = {k: v for k, v in activation_diff.items() if np.isscalar(v)}\n",
    "\n",
    "# Convert the filtered dictionary to a Pandas Series\n",
    "activation_diff_series = pd.Series(activation_diff_filtered)\n",
    "\n",
    "# Now you can perform the sorting and selection\n",
    "top_neurons = activation_diff_series.abs().sort_values(ascending=False).head(10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we're interested in the activation difference between class 1 and class 0\n",
    "activation_diff_series = activation_diff['class_1_minus_class_0']\n",
    "\n",
    "# Get top neurons with the largest activation differences\n",
    "top_neurons = activation_diff_series.abs().sort_values(ascending=False).head(10).index\n",
    "\n",
    "# Map neuron names to global neuron IDs\n",
    "top_global_neuron_ids = []\n",
    "for neuron_name in top_neurons:\n",
    "    layer_name, neuron_label = neuron_name.split('_neuron_')\n",
    "    neuron_index = int(neuron_label)\n",
    "    global_neuron_id = network_trace.global_neuron_id_map.get((layer_name, neuron_index))\n",
    "    if global_neuron_id is not None:\n",
    "        top_global_neuron_ids.append(global_neuron_id)\n",
    "\n",
    "# In your network visualization, highlight these neurons\n",
    "highlighted_nodes = set()\n",
    "for global_neuron_id in top_global_neuron_ids:\n",
    "    node_name = f\"{layer_name}_{global_neuron_id}\"  # Adjust if necessary\n",
    "    highlighted_nodes.add(node_name)\n",
    "\n",
    "# Modify visualization to highlight these nodes\n",
    "node_colors = []\n",
    "for node in G.nodes():\n",
    "    if node in highlighted_nodes:\n",
    "        node_colors.append('yellow')  # Highlight color\n",
    "    else:\n",
    "        node_colors.append('lightgray')\n",
    "\n",
    "# Draw nodes with updated colors\n",
    "nx.draw_networkx_nodes(G, pos, node_size=500, node_color=node_colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
