{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trace_nn import NetworkTrace, TraceObject\n",
    "from model import SimpleNN\n",
    "import torch\n",
    "import pickle\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "# Run the main process in main.py\n",
    "subprocess.run([\"python\", \"main.py\"])\n",
    "\n",
    "# Load the network trace data saved by main.py\n",
    "with open('outputs/network_trace.pkl', 'rb') as f:\n",
    "    network_trace = pickle.load(f)\n",
    "# Confirm trace data loaded\n",
    "print(f\"Trace data loaded with epochs: {list(network_trace.trace.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing \n",
    "We want to prepare our recorded network_trace schema data by converting it into suitable formats for analysis.\n",
    "First, we define a function `create_final_classification_series` which converts final classifcation results into a list for target variable y. \n",
    "\n",
    "We then define `convert_nn_schema_to_df` which converts our network trace schema into a multi-indexed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to get the data for our y variable, which is all the final classification results \n",
    "\n",
    "def create_final_classification_series(network_trace):\n",
    "    # Convert the final classification results dictionary to a flattened list\n",
    "    final_results = network_trace.final_classification_results\n",
    "\n",
    "    # Initialize an empty list to hold all flattened classification results\n",
    "    flattened_results = []\n",
    "\n",
    "    # Iterate over the epochs and flatten the classification results\n",
    "    for epoch, result in final_results.items():\n",
    "        if isinstance(result, torch.Tensor):\n",
    "            # Convert the tensor to a numpy array, flatten it, and extend the flattened_results list\n",
    "            flattened_results.extend(result.cpu().numpy().flatten().tolist())\n",
    "        else:\n",
    "            flattened_results.extend(result)\n",
    "\n",
    "    # Convert the flattened list to a pandas Series\n",
    "    series = pd.Series(flattened_results, name=\"Class_Result\")\n",
    "\n",
    "    return series\n",
    "\n",
    "# Example usage\n",
    "classification_series = create_final_classification_series(network_trace)\n",
    "print(classification_series, 'hi')\n",
    "\n",
    "# Example usage\n",
    "classification_series = create_final_classification_series(network_trace)\n",
    "display(classification_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from helpers import get_model_level_integer\n",
    "\n",
    "def convert_nn_schema_to_df(network_trace):\n",
    "    # Create a list to hold the data for each neuron\n",
    "    data = []\n",
    "    # Create a list to hold the multi-level index (epoch, layer, neuron)\n",
    "    index = []\n",
    "\n",
    "    for epoch, layers in network_trace.trace.items():\n",
    "        for layer_name, neurons in layers.items():\n",
    "            layer_weights = network_trace.weights[epoch][layer_name]\n",
    "            \n",
    "            # Convert the weights tensor to a numpy array if it's a tensor\n",
    "            if isinstance(layer_weights, torch.Tensor):\n",
    "                layer_weights = layer_weights.numpy()\n",
    "            \n",
    "            num_input_connections = layer_weights.shape[1]  # This determines the number of input connections\n",
    "\n",
    "            for neuron_id, neuron_data in neurons.items():\n",
    "                neuron_id_int = get_model_level_integer(neuron_id) % layer_weights.shape[0]\n",
    "                \n",
    "                if neuron_id_int >= len(layer_weights):\n",
    "                    print(f\"Index {neuron_id_int} out of bounds for layer {layer_name} with size {len(layer_weights)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract the weights for this specific neuron\n",
    "                weight_for_neuron = layer_weights[neuron_id_int].tolist()\n",
    "                \n",
    "                # Pad the weights with None or a small number to ensure consistent column size\n",
    "                weight_for_neuron.extend([0.000001] * (num_input_connections - len(weight_for_neuron)))\n",
    "                \n",
    "                # Append the weights to the data list\n",
    "                data.append(weight_for_neuron)\n",
    "                # Append the index (epoch, layer, neuron_id)\n",
    "                index.append((epoch, layer_name, neuron_id))\n",
    "\n",
    "    # Create the MultiIndex\n",
    "    multi_index = pd.MultiIndex.from_tuples(index, names=[\"Epoch\", \"Layer\", \"Neuron\"])\n",
    "\n",
    "    # Create the DataFrame and return it with NaNs filled with a small number\n",
    "    df = pd.DataFrame(data, index=multi_index)\n",
    "    return df.fillna(0.000001)\n",
    "model_df = convert_nn_schema_to_df(network_trace)\n",
    "display(model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to do about NaNs\n",
    "At this point the question of what to do about missing values comes up.  For a fully connected network, the shape of the weight tensor is `(output_features, input_features)`. Unless a network is a perfect cube with all layers having the same number of output features and input features, this will result in empty values for neurons in layers that have less input features than the layer with the maximum number of input features. \n",
    "\n",
    "Thus, in the example of \n",
    "L_1 = {n_0, ..., n_19} with  shape: (10,20)\n",
    "L_2 = {n_20, ... n_29}  with shape: (10, 10)\n",
    "L_3 = {n_30} with shape: (1, 10)\n",
    "... \n",
    "Layer 2 (L_2) will have 10 fewer values per neuron than its previous layer, as it will have 10 fewer input connections. \n",
    "\n",
    "This makes sense intuitively because a layer with 20 neurons will take 20 input connections but give 10 connections to every neuron in the next layer with 10 neurons. But if the next layer only has 10 neurons, it will only pass 10 connections to the next layer.\n",
    "\n",
    "Something must thus be done about the holes in the data for all smaller layers  as many ML algorithms will not work with NaNs. \n",
    "\n",
    "One thought is to impute -1 to the NaNs, and then perform some operation later to \"cancel out\" these filler calculations. This would involve mathematical techniques that are currently unclear. \n",
    "\n",
    "So for now, I have chosen to replace the NaNs with a very improbable small value, understanding that while this may bias the models somewhat, the damage is minimal and the grid is perserved with little added complication. \n",
    "\n",
    "This will be revisted in future iterations if better methods are discovered. \n",
    "\n",
    "### Column alignment problem for the neuron level\n",
    "Since a dataframe is \"right angled\", the largest layer in the network will define an upper boundary for the features. This means only the layer with 20 input neurons will have a full table in the dataframe and all layers in the network will have empty values where the difference between its size and the max layer size would be.\n",
    "\n",
    " If we were to index the dataframe to the max_neurons, we would have a sparser table.  \n",
    "\n",
    " So it is important to remember that columns 0-19 at the neuron level are  *not a universal point of reference*.\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Model Metadata \n",
    "\n",
    "\n",
    "\n",
    "### Meta-model training \n",
    "This next phase involves taking the data from the model and modeling it. This is to help us select which neurons are the most associated with given outputs. We will then use that data to identify our model's proposed \"structures\". \n",
    "\n",
    "The idea is to use machine learning *reflectively* on the results of machine learning, to see if we can find meaningful patterns in the numbers it produces.\n",
    "\n",
    "#### What is a structure?\n",
    "The central thesis of this experiment is that we can gain clarity about a model's inner workings by identifing what nodes in the network participate most significantly in contributing to certain outputs. \n",
    "\n",
    "If we can identify the \"where\" (by neural id and layer) and \"when\" (by epoch, or perhaps in future iterations, batch id) these neurons contribute to certain outputs, we can define a structure associated with that ouput.\n",
    "\n",
    "This structure would be analogous to identifying what part of the brain is involved in producing language, or recognizing certain shapes, or perceiving sounds, etc. \n",
    "\n",
    "These structures are effectively the embodiment of the model's \"concept\" of the output. \n",
    "\n",
    "With the identification of structures, we can then label them, just as we label regions of the brain (Broca's area, the anterior cingulate gyrus, etc). And once we label something, we can begin to reference it, it has a chance to become something we can place programmatic hooks into. We could theoretically delete or change these structures programatically once we identify them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods of Analysis \n",
    "There is room here for any type of analysis and discretion can be left up to the researcher. The idea is to experiment and find out.\n",
    "\n",
    "Some options might include:\n",
    "\n",
    "- Time-series analysis (by observing changes across neural populations or layers across epochs)\n",
    "- Layer-wise analysis (by observing changes in state of a layer or interactions between layers)\n",
    "- Aggregate neural analysis (identify pairwise connections, strong correlations between neurons or profile a single neuron's metrics )\n",
    "- Heatmaps, clustering\n",
    "\n",
    "The ultimate goal is to apply various machine learning methods experimentally to see if any meaningful patterns can be discovered in network metadata. \n",
    "\n",
    "Below will be a sampling of various methods that will periodically be added and expanded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aggregate_layers_by_type(df):\n",
    "    # Create a dictionary to store DataFrames for each layer type\n",
    "    layer_dfs = {}\n",
    "\n",
    "    # Get all unique layer types\n",
    "    layer_types = df.index.get_level_values('Layer').unique()\n",
    "    # Iterate over each layer type\n",
    "    for layer in layer_types:\n",
    "        layer_df = df.xs(layer, level='Layer')\n",
    "        layer_dfs[layer] = layer_df\n",
    "\n",
    "    return layer_dfs\n",
    "\n",
    "def aggregate_neurons_across_layers(layer_dfs):\n",
    "    # Concatenate all DataFrames in layer_dfs\n",
    "    concatenated_df = pd.concat(layer_dfs.values())\n",
    "\n",
    "    # Extract neuron types from the index\n",
    "    concatenated_df.index = concatenated_df.index.map(lambda x: x[1])  # Assuming (Epoch, Neuron) is the index\n",
    "    \n",
    "    # Group by neuron type and aggregate\n",
    "    aggregated_df = concatenated_df.groupby(level=0).mean()  # You can replace mean() with another aggregation method\n",
    "\n",
    "    return aggregated_df\n",
    "\n",
    "\n",
    "# def aggregate_neuron_by_type(df):\n",
    "    # # Ditto neurons \n",
    "    # neuron_df = {}\n",
    "    # print(df)\n",
    "    # neuron_types = df.index.get_level_values('Neuron').unique()\n",
    "\n",
    "    # for neuron in neuron_types:\n",
    "    #     neuron_df = df.xs(neuron, level='neuron')\n",
    "    # return neuron_df\n",
    "\n",
    "\n",
    "layer_dfs = aggregate_layers_by_type(model_df)\n",
    "neuron_dfs = aggregate_neurons_across_layers(layer_dfs)\n",
    "display(neuron_dfs)\n",
    "\n",
    "L_input_df = layer_dfs['L_input']\n",
    "L_hidden_1_df = layer_dfs['L_hidden_1']\n",
    "L_output_df = layer_dfs['L_output']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Aggregate by taking the mean across all epochs for the L_input layer\n",
    "# L_input_avg = L_input_df.groupby(level='Neuron').mean()\n",
    "# print('avesss', L_input_avg, 'ave ')\n",
    "# Display the aggregated DataFrame\n",
    "\n",
    "def compute_epoch_average(df):\n",
    "    # Group by Layer and Neuron to compute averages across all epochs\n",
    "    average_df = df.groupby(level=[\"Layer\", \"Neuron\"]).mean()\n",
    "\n",
    "    # Create a new index for \"Epoch_average\" with the same Layer and Neuron combinations\n",
    "    average_df = average_df.reset_index()\n",
    "    average_df['Epoch'] = 'Epoch_average'\n",
    "    average_df.set_index(['Epoch', 'Layer', 'Neuron'], inplace=True)\n",
    "\n",
    "    # Reset the index of the original dataframe to prepare for merging\n",
    "    df_reset = df.reset_index()\n",
    "\n",
    "    # Combine the original data and the average data\n",
    "    final_df = pd.concat([df_reset, average_df.reset_index()])\n",
    "\n",
    "    # Set the index back to [\"Epoch\", \"Layer\", \"Neuron\"]\n",
    "    final_df.set_index(['Epoch', 'Layer', 'Neuron'], inplace=True)\n",
    "\n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3100, 20)\n",
      "(695,)\n",
      "[[ 9.79504436e-02  1.63503379e-01 -4.78580929e-02 ... -1.17831320e-01\n",
      "   6.76362962e-03  3.79307531e-02]\n",
      " [ 6.86960295e-02 -6.44231960e-02 -9.53575671e-02 ...  1.14299737e-01\n",
      "  -5.61272912e-02  1.80215184e-02]\n",
      " [ 5.07868789e-02 -1.65503025e-01  1.91396311e-01 ...  1.35015815e-01\n",
      "  -1.39886677e-01  1.41200006e-01]\n",
      " ...\n",
      " [ 1.34311736e-01 -3.70444991e-02  1.34768173e-01 ...  1.00000000e-06\n",
      "   1.00000000e-06  1.00000000e-06]\n",
      " [-3.37016433e-01 -7.64986575e-02  2.09974930e-01 ...  1.00000000e-06\n",
      "   1.00000000e-06  1.00000000e-06]\n",
      " [-1.77767456e-01 -3.19150746e-01  1.85033649e-01 ...  1.00000000e-06\n",
      "   1.00000000e-06  1.00000000e-06]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3100, 695]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Step 3: Train/Test Split\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Step 4: Train the model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_dev/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_dev/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2657\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2661\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2662\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_dev/lib/python3.12/site-packages/sklearn/utils/validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <3x1 sparse matrix ...>]\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 514\u001b[0m check_consistent_length(\u001b[38;5;241m*\u001b[39mresult)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ai_dev/lib/python3.12/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3100, 695]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(model_df.shape)\n",
    "print(classification_series.shape)\n",
    "\n",
    "# Now convert the DataFrame to the feature matrix `X` and target `y`\n",
    "X = model_df.values  # Assuming the features are in the DataFrame\n",
    "y = classification_series\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train the model\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 6: Analyze coefficients\n",
    "coefficients = model.coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
