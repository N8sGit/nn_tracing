{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TracableNN\n",
    "from model_config import config\n",
    "import torch\n",
    "import pickle\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main process in main.py to execute the training loop and save the model's learnable parameters\n",
    "subprocess.run([\"python\", \"main.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model structure (without re-training)\n",
    "# This is necessary to run the model in inference mode later\n",
    "input_size = config['input_size']\n",
    "hidden_size = config['hidden_size']\n",
    "output_size = config['output_size']\n",
    "num_epochs = config['num_epochs']\n",
    "model = TracableNN(input_size, hidden_size, output_size, num_epochs)\n",
    "\n",
    "# Load the trained model's state\n",
    "model.load_state_dict(torch.load('outputs/trained_model.pth'))\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with open('outputs/network_trace.pkl', 'rb') as f:\n",
    "    network_trace = pickle.load(f)\n",
    "model.network_trace = network_trace\n",
    "\n",
    "\n",
    "# Check if the trace has data\n",
    "if network_trace.trace:\n",
    "    print(f\"Trace data loaded with epochs: {list(network_trace.trace.keys())}\")\n",
    "else:\n",
    "    print(\"Error: Trace data is empty or not loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing \n",
    "We want to prepare our recorded network_trace schema data by converting it into a dataframe suitable for analysis and computation.\n",
    "First, we define a function `create_final_classification_series` which converts final classifcation results into a list for target variable y. \n",
    "We then define `convert_nn_schema_to_df` which converts our network trace schema into a multi-indexed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to get the data for our y variable, which is all the final classification results \n",
    "def create_final_classification_series(network_trace):\n",
    "    # Convert the final classification results dictionary to a flattened list\n",
    "    final_results = network_trace.final_classification_results\n",
    "\n",
    "    # Initialize an empty list to hold all flattened classification results\n",
    "    flattened_results = []\n",
    "\n",
    "    # Iterate over the epochs and flatten the classification results\n",
    "    for epoch, result in final_results.items():\n",
    "        if isinstance(result, torch.Tensor):\n",
    "            # Convert the tensor to a numpy array, flatten it, and extend the flattened_results list\n",
    "            flattened_results.extend(result.cpu().numpy().flatten().tolist())\n",
    "        else:\n",
    "            flattened_results.extend(result)\n",
    "\n",
    "    # Convert the flattened list to a pandas Series\n",
    "    series = pd.Series(flattened_results, name=\"Class_Result\")\n",
    "\n",
    "    return series\n",
    "\n",
    "# Example usage\n",
    "classification_series = create_final_classification_series(network_trace)\n",
    "\n",
    "# Example usage\n",
    "classification_series = create_final_classification_series(network_trace)\n",
    "display(classification_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_model_level_integer\n",
    "\n",
    "def convert_nn_schema_to_df(network_trace):\n",
    "    # Create a list to hold the data for each neuron\n",
    "    data = []\n",
    "    # Create a list to hold the multi-level index (epoch, layer, neuron)\n",
    "    index = []\n",
    "\n",
    "    # Retrieve final classification results\n",
    "    final_classification_results = network_trace.final_classification_results\n",
    "\n",
    "    for epoch, layers in network_trace.trace.items():\n",
    "        for layer_name, neurons in layers.items():\n",
    "            layer_weights = network_trace.weights[epoch][layer_name]\n",
    "            \n",
    "            # Convert the weights tensor to a numpy array if it's a tensor\n",
    "            if isinstance(layer_weights, torch.Tensor):\n",
    "                layer_weights = layer_weights.numpy()\n",
    "            \n",
    "            num_input_connections = layer_weights.shape[1]  # This determines the number of input connections\n",
    "\n",
    "            for neuron_id, neuron_data in neurons.items():\n",
    "                neuron_id_int = get_model_level_integer(neuron_id) % layer_weights.shape[0]\n",
    "                \n",
    "                if neuron_id_int >= len(layer_weights):\n",
    "                    print(f\"Index {neuron_id_int} out of bounds for layer {layer_name} with size {len(layer_weights)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract the weights for this specific neuron\n",
    "                weight_for_neuron = layer_weights[neuron_id_int].tolist()\n",
    "                \n",
    "                # Pad the weights with None or a small number to ensure consistent column size\n",
    "                weight_for_neuron.extend([0.00000] * (num_input_connections - len(weight_for_neuron)))\n",
    "                \n",
    "                # Append the weights to the data list\n",
    "                data.append(weight_for_neuron)\n",
    "                # Append the index (epoch, layer, neuron_id)\n",
    "                index.append((epoch, layer_name, neuron_id))\n",
    "\n",
    "    # Create the MultiIndex\n",
    "    multi_index = pd.MultiIndex.from_tuples(index, names=[\"Epoch\", \"Layer\", \"Neuron\"])\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(data, index=multi_index)\n",
    "\n",
    "    # Retrieve final classification results for each epoch and map to DataFrame\n",
    "    classification_series = pd.Series(final_classification_results).map(lambda x: x.cpu().numpy().flatten()[0] if isinstance(x, torch.Tensor) else x)\n",
    "    classification_series.name = \"Final_Classification_Result\"\n",
    "\n",
    "    # Join the classification results to the main DataFrame\n",
    "    df = df.join(classification_series, on=\"Epoch\")\n",
    "\n",
    "    # Return the DataFrame with NaNs filled with a small number\n",
    "    return df.fillna(0.000000)\n",
    "model_df = convert_nn_schema_to_df(network_trace)\n",
    "display(model_df[0:31])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exampledf = model_df[0:31]\n",
    "# from pathlib import Path  \n",
    "# filepath = Path('outputs/data_slice.csv')  \n",
    "# filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "# exampledf.to_csv(filepath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to do about non-orthogonality\n",
    "For a typical fully connected network, the shape of the weight tensor is `(output_features, input_features)`. Unless a network is a perfect cube (with all layers having the same number of output features and input features) a dataframe of a network schema will result in empty values for neurons in layers that have less input features than the layer with the maximum number of input features. \n",
    "\n",
    "So in the example of \n",
    "L_1 = {n_0, ..., n_19} with  shape: (10,20)\n",
    "L_2 = {n_20, ... n_29}  with shape: (10, 10)\n",
    "L_3 = {n_30} with shape: (1, 10)\n",
    "... \n",
    "Layer 2 (L_2) will have 10 fewer values per neuron than its previous layer, as it will have 10 fewer input connections. \n",
    "\n",
    "This makes sense intuitively because a layer with 20 neurons will take 20 input connections but give 10 connections to every neuron in the next layer with 10 neurons. But if the next layer only has 10 neurons, it will only pass 10 connections to the next layer.\n",
    "\n",
    "Something must thus be done about the holes in the data for all smaller layers  as many ML algorithms will not work with NaNs. \n",
    "\n",
    "Imputing 0 with for the NaNs probably makes the most sense since it effectively means that value is nothing\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Column alignment problem for the neuron level\n",
    "\n",
    "Since a dataframe is \"right angled\", the largest layer in the network will define an upper boundary for the features. This means only the layer with 20 input neurons will have a full table in the dataframe and all layers in the network will have empty values where the difference between its size and the max layer size would be.\n",
    "\n",
    " If we were to index the dataframe to the max_neurons, we would have sparse table that is difficult to work with.  \n",
    "\n",
    " So it is important to remember that columns 0-19 at the neuron level are  *not a universal point of reference*.\n",
    " We have to be careful how we calculate our tables so that we aren't mistakenly comparing values at different locations on the network but that have congruent columns on the dataframe. \n",
    "\n",
    " Later, we will repackage our weights into tuples to keep track of their actual associations. \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "### Analyzing Model Metadata \n",
    "\n",
    "#### Meta-model training \n",
    "This next phase involves taking the data from the model and modeling it. This is to help us select which neurons are the most associated with given outputs. We will then use that data to identify our model's proposed \"structures\". \n",
    "\n",
    "The idea is to use machine learning *reflectively* on the results of machine learning, to see if we can find meaningful patterns in the numbers it produces.\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "### Structure Identification\n",
    "\n",
    "#### What is a structure? \n",
    "The central thesis of this experiment is that we can gain clarity about a model's inner workings by identifing what nodes in the network participate most significantly in contributing to certain outputs. \n",
    "\n",
    "If we can identify the \"where\" (by neural id and layer) and \"when\" (by epoch, or perhaps in future iterations, batch id) these neurons contribute to certain outputs, we can define a structure associated with that ouput.\n",
    "\n",
    "This structure would be analogous to identifying what part of the brain is involved in producing language, or recognizing certain shapes, or perceiving sounds, etc. \n",
    "\n",
    "These structures are effectively the embodiment of the model's \"concept\" of the output. \n",
    "\n",
    "With the identification of structures, we can then label them, just as we label regions of the brain (Broca's area, the anterior cingulate gyrus, etc). And once we label something, we can begin to reference it, it has a chance to become something we can place programmatic hooks into. We could theoretically delete or change these structures programatically once we identify them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "### Methods of Analysis \n",
    "There is room here for any type of analysis that can be left to discretion of the researcher. At first glance it's not clear what approach would be most insightful. The idea is to experiment and find out.\n",
    "\n",
    "Some options might include:\n",
    "\n",
    "- Time-series analysis (by observing changes across neural populations or layers across epochs)\n",
    "- Layer-wise analysis (by observing changes in state of a layer or interactions between layers)\n",
    "- Aggregate neural analysis (identify pairwise connections, strong correlations between neurons or profile a single neuron's metrics )\n",
    "- Markov Chains or Hidden Markov Models \n",
    "- Heatmaps, clustering, correlational analysis\n",
    "\n",
    "The ultimate goal is to apply various machine learning methods experimentally to see if any meaningful patterns can be discovered in network metadata. \n",
    "\n",
    "Below will be a sampling of various methods that will be periodically added to and expanded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def aggregate_layers_by_type(df):\n",
    "    # Create a dictionary to store DataFrames for each layer type\n",
    "    layer_dfs = {}\n",
    "\n",
    "    # Get all unique layer types\n",
    "    layer_types = df.index.get_level_values('Layer').unique()\n",
    "    \n",
    "    # Iterate over each layer type\n",
    "    for layer in layer_types:\n",
    "        layer_df = df.xs(layer, level='Layer')\n",
    "        \n",
    "        # Ensure that the DataFrame contains only numerical values (ints or floats)\n",
    "        layer_df = layer_df.applymap(lambda x: x if isinstance(x, (int, float)) else 0)\n",
    "        \n",
    "        layer_dfs[layer] = layer_df\n",
    "\n",
    "    return layer_dfs\n",
    "\n",
    "def aggregate_neurons_across_layers(layer_dfs):\n",
    "    # Concatenate all DataFrames in layer_dfs\n",
    "    concatenated_df = pd.concat(layer_dfs.values())\n",
    "\n",
    "    # Extract neuron types from the index\n",
    "    concatenated_df.index = concatenated_df.index.map(lambda x: x[1])  \n",
    "    \n",
    "    # Ensure that the DataFrame contains only numerical values (ints or floats)\n",
    "    concatenated_df = concatenated_df.applymap(lambda x: x if isinstance(x, (int, float)) else 0)\n",
    "    \n",
    "    # Group by neuron type and aggregate (e.g., mean, sum, etc.)\n",
    "    aggregated_df = concatenated_df.groupby(level=0).mean()\n",
    "\n",
    "    return aggregated_df\n",
    "\n",
    "\n",
    "layer_dfs = aggregate_layers_by_type(model_df)\n",
    "neuron_dfs = aggregate_neurons_across_layers(layer_dfs)\n",
    "\n",
    "display(neuron_dfs)\n",
    "\n",
    "L_input_df = layer_dfs['L_input']\n",
    "L_hidden_1_df = layer_dfs['L_hidden_1']\n",
    "L_output_df = layer_dfs['L_output']\n",
    "\n",
    "def compute_epoch_average(df):\n",
    "    # Group by Layer and Neuron to compute averages across all epochs\n",
    "    average_df = df.groupby(level=[\"Layer\", \"Neuron\"]).mean()\n",
    "\n",
    "    # Create a new index for \"Epoch_average\" with the same Layer and Neuron combinations\n",
    "    average_df = average_df.reset_index()\n",
    "    average_df['Epoch'] = 'Epoch_average'\n",
    "    average_df.set_index(['Epoch', 'Layer', 'Neuron'], inplace=True)\n",
    "\n",
    "    # Reset the index of the original dataframe to prepare for merging\n",
    "    df_reset = df.reset_index()\n",
    "\n",
    "    # Combine the original data and the average data\n",
    "    final_df = pd.concat([df_reset, average_df.reset_index()])\n",
    "\n",
    "    # Set the index back to [\"Epoch\", \"Layer\", \"Neuron\"]\n",
    "    final_df.set_index(['Epoch', 'Layer', 'Neuron'], inplace=True)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "t-SNE is a machine learning algorithm used for dimensionality reduction. It plots high dimensional objects on a lower-dimensional map and clusters them based by similarity using distance on the plot as a measure of difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Ensure all column names are strings\n",
    "neuron_dfs.columns = neuron_dfs.columns.astype(str)\n",
    "\n",
    "X = neuron_dfs\n",
    "neuron_ids = neuron_dfs.index\n",
    "X_tsne = TSNE(n_components=3, random_state=42).fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "\n",
    "texts = []\n",
    "for i, neuron_id in enumerate(neuron_ids):\n",
    "    x_offset = 0.5 * (1 if np.random.rand() > 0.5 else -1)\n",
    "    y_offset = 0.5 * (1 if np.random.rand() > 0.5 else -1)\n",
    "    text = plt.text(X_tsne[i, 0] + x_offset, X_tsne[i, 1] + y_offset, str(neuron_id), fontsize=8)\n",
    "    texts.append(text)\n",
    "\n",
    "# Automatically adjust the text annotations to avoid overlaps\n",
    "adjust_text(texts, only_move={'points':'y', 'text':'xy'}, arrowprops=dict(arrowstyle=\"-\", color='gray', lw=0.5))\n",
    "\n",
    "plt.title('t-SNE visualization of neuron activations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: \n",
    "While the above plot doesn't give us much insight, it does show there is some underlying structural grouping of neurons which may provide the skeleton for a deeper analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test/split the data\n",
    "\n",
    "# Run model.trace_inference() over using X_test over num_epoch trials (100)\n",
    "\n",
    "# Separate data sets when prediction is correct versus incorrect\n",
    "\n",
    "# Compare distances between correct and incorrect\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise association of neural connections (WIP)\n",
    "\n",
    "\"Neurons the fire together, wire together\"\n",
    "This famous rule of thumb belongs to the realm of Hebbian learning in neurobiology. Does it also apply in the artifical context? \n",
    "\n",
    "One way about this is to try to find the neurons that are most closely associated and co-activated.\n",
    "\n",
    "Prior to constructing the visualizations we should reorganize the weights so that we have a more maneuverable interface for tracing the connections between neurons.\n",
    "\n",
    "As mere numbers, the weights aren't very human readable or traversable. But each weight contains a pair of information, that can be represented as a 2-tuple containing the following information\n",
    "``(neuron_connected_to_current_neuron, strength_of_connection)``\n",
    "In order to finally extract the neurons that are the most closely associated, we need to first make these connections more manifest. So we update our weights to be have these tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_df.columns)\n",
    "def map_weights_input(df):\n",
    "    # Create a dictionary to map the previous layer neuron IDs\n",
    "    # TO DO: IMPORT THESE SETTINGS FROM A SINGLE SOURCE OF TRUTH TO REMOVE HARD CODING \n",
    "    previous_layer_mapping = {\n",
    "        \"L_input\": [f\"input_{i}\" for i in range(20)],  \n",
    "        \"L_hidden_1\": [f\"n_{i}\" for i in range(10)],  \n",
    "        \"L_output\": [f\"n_{i+10}\" for i in range(10)],\n",
    "    }\n",
    "\n",
    "    def wrap_with_input_neuron(row, layer):\n",
    "        previous_neurons = previous_layer_mapping[layer]\n",
    "        # For weights that exceed the number of neurons in the previous layer, set the index part to None\n",
    "        return [(previous_neurons[i] if i < len(previous_neurons) else None, val)\n",
    "                for i, val in enumerate(row)]\n",
    "\n",
    "    decorated_data = []\n",
    "    for (epoch, layer, neuron), row in df.iterrows():\n",
    "        # Check for columns to exclude from the mapping\n",
    "        if neuron == any([\"Final_Classification_Result\"]):\n",
    "            decorated_data.append(row.tolist())  # Keep the original data\n",
    "        else:\n",
    "            wrapped_row = wrap_with_input_neuron(row, layer)\n",
    "            decorated_data.append(wrapped_row)\n",
    "\n",
    "    # Reconstruct DataFrame with decorated values\n",
    "    decorated_df = pd.DataFrame(decorated_data, index=df.index, columns=df.columns)\n",
    "\n",
    "    return decorated_df\n",
    "\n",
    "# Assuming the output of convert_nn_schema_to_df is stored in `model_df`\n",
    "decorated_df = map_weights_input(model_df)\n",
    "display(decorated_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
